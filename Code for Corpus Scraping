import re
import time
from pathlib import Path

import praw
from prawcore.exceptions import RequestException, ResponseException, ServerError
from docx import Document

# ============================
#   CONFIGURATION
# ============================

# >>>> PUT YOUR CREDENTIALS HERE <<<<
CLIENT_ID = "YOUR_CLIENT_ID"
CLIENT_SECRET = "YOUR_CLIENT_SECRET"
USER_AGENT = "bipolar_corpus_research_script_by_u_yourusername"

SUBREDDIT_NAME = "depression"

# How many posts to attempt (None = as many as Reddit provides)
POST_LIMIT = None  # or set e.g. 5000 if you want a cap

# Output files (in current directory)
OUTPUT_TXT = Path("depression_corpus.txt")
OUTPUT_DOCX = Path("depression_corpus.docx")


# ============================
#   HELPER FUNCTIONS
# ============================

def clean_text(text: str) -> str:
    """
    Remove URLs, extra whitespace. Keep plain text only.
    """
    if not text:
        return ""
    # Remove URLs
    text = re.sub(r"http\S+|www\.\S+", " ", text)
    # Remove excessive whitespace
    text = re.sub(r"\s+", " ", text).strip()
    return text


def safe_sleep(seconds: int):
    """
    Sleep helper (for clarity).
    """
    print(f"Sleeping for {seconds} seconds to avoid timeout/rate issues...")
    time.sleep(seconds)


# ============================
#   MAIN SCRAPER
# ============================

def scrape_subreddit():
    # Initialise Reddit instance
    reddit = praw.Reddit(
        client_id=CLIENT_ID,
        client_secret=CLIENT_SECRET,
        user_agent=USER_AGENT,
    )

    subreddit = reddit.subreddit(SUBREDDIT_NAME)

    all_text_chunks = []  # we'll store pieces of text here

    print(f"Starting scrape of r/{SUBREDDIT_NAME} ...")

    # Iterate over posts (submissions)
    # Using .new() – you could switch to .top() or .hot() if you prefer
    try:
        submissions = subreddit.new(limit=POST_LIMIT)
    except Exception as e:
        print("Error creating submissions generator:", e)
        return

    post_count = 0

    for submission in submissions:
        post_count += 1
        print(f"Processing post #{post_count}: {submission.title[:60]!r}")

        # Wrap each submission processing in try/except so we don't die on one error
        try:
            # Extract post text (title + selftext)
            title = clean_text(submission.title)
            body = clean_text(submission.selftext)

            post_text_parts = []

            if title:
                post_text_parts.append(title)
            if body:
                post_text_parts.append(body)

            # Fetch all comments (expand "MoreComments")
            try:
                submission.comments.replace_more(limit=None)
            except (RequestException, ResponseException, ServerError) as e:
                print(f"Error expanding comments on post #{post_count}: {e}")
                safe_sleep(10)

            for comment in submission.comments.list():
                if comment.body in ("[deleted]", "[removed]"):
                    continue
                comment_text = clean_text(comment.body)
                if comment_text:
                    post_text_parts.append(comment_text)

            # Combine everything from this thread into one big block
            if post_text_parts:
                thread_text = "\n".join(post_text_parts)
                all_text_chunks.append(thread_text)

        except (RequestException, ResponseException, ServerError) as e:
            # Network / Reddit API issues – wait and continue
            print(f"Network/API error on post #{post_count}: {e}")
            safe_sleep(20)
            continue
        except Exception as e:
            # Catch-all so one weird post doesn't kill the scrape
            print(f"Unexpected error on post #{post_count}: {e}")
            continue

    print(f"\nFinished iterating posts. Total threads collected: {len(all_text_chunks)}")
    return all_text_chunks


def save_as_txt(text_chunks, output_path: Path):
    """
    Save all collected text chunks into a single .txt corpus file.
    Each thread separated by a blank line.
    """
    print(f"Saving text corpus to: {output_path}")
    with output_path.open("w", encoding="utf-8") as f:
        for chunk in text_chunks:
            f.write(chunk)
            f.write("\n\n")  # blank line between threads
    print("Text corpus saved.")


def save_as_docx(text_chunks, output_path: Path):
    """
    Save all collected text chunks into a Word document.
    Each thread as a separate paragraph.
    """
    print(f"Saving Word corpus to: {output_path}")
    doc = Document()
    doc.add_heading(f"Reddit r/{SUBREDDIT_NAME} Corpus", level=1)

    for i, chunk in enumerate(text_chunks, start=1):
        doc.add_paragraph(chunk)
        # optional: add a separator line between threads
        doc.add_paragraph("-" * 40)

    doc.save(output_path)
    print("Word corpus saved.")


# ============================
#   ENTRY POINT
# ============================

def main():
    text_chunks = scrape_subreddit()
    if not text_chunks:
        print("No data scraped. Exiting.")
        return

    save_as_txt(text_chunks, OUTPUT_TXT)
    save_as_docx(text_chunks, OUTPUT_DOCX)
    print("\n✅ All done!")


if __name__ == "__main__":
    main()
